#!/usr/bin/env python3
import argparse
import fcntl
import logging
import os
import signal
import subprocess
import sys
import threading
import time
from collections import deque
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

VERSION = "1.0.1"
BASE_DIR = Path("/tmp/orun")
BASE_DIR.mkdir(exist_ok=True, mode=0o777, parents=True)
MAX_RETRIES = 3
LOCK_TIMEOUT = 5  # seconds


class GPUManager:
    @staticmethod
    def _check_nvidia_smi():
        """检查 nvidia-smi 可用性"""
        try:
            subprocess.run(
                ["nvidia-smi", "--version"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                timeout=2,
                check=True,
            )
            return True
        except (subprocess.SubprocessError, FileNotFoundError):
            logger.warning("未检测到 NVIDIA 驱动或 nvidia-smi 工具")
            return False

    @staticmethod
    def get_gpu_status():
        """获取 GPU 状态信息"""
        if not GPUManager._check_nvidia_smi():
            return []

        def run_query(fields, query_type):
            try:
                output = (
                    subprocess.check_output(
                        [
                            "nvidia-smi",
                            f"--query-{query_type}={fields}",
                            "--format=csv,noheader,nounits",
                        ],
                        timeout=5,
                    )
                    .decode()
                    .strip()
                )
                return output.split("\n") if output else []
            except subprocess.TimeoutExpired:
                logger.error("nvidia-smi 查询超时")
                return []
            except subprocess.SubprocessError as e:
                logger.error(f"执行 nvidia-smi 失败: {e}")
                return []

        try:
            # 获取 GPU 基础信息
            gpu_fields = "index,utilization.gpu,memory.used,memory.total,gpu_bus_id"
            gpu_data = run_query(gpu_fields, "gpu")

            # 获取进程占用信息
            process_fields = "pid,gpu_bus_id"
            process_data = run_query(process_fields, "compute-apps")

            # 构建总线 ID 到进程 PID 的映射
            bus_id_to_pids = {}
            for line in process_data:
                parts = line.strip().split(", ")
                if len(parts) == 2:
                    try:
                        pid = int(parts[0])
                        bus_id = parts[1]
                        bus_id_to_pids.setdefault(bus_id, []).append(pid)
                    except ValueError:
                        logger.warning(f"解析进程信息失败: {line}")
                        continue

            # 解析 GPU 数据
            gpus = []
            for line in gpu_data:
                parts = line.strip().split(", ")
                if len(parts) != 5:
                    logger.warning(f"解析 GPU 信息失败 (字段数量不匹配): {line}")
                    continue
                try:
                    gpu_info = {
                        "index": int(parts[0]),
                        "utilization": int(parts[1]),
                        "mem_used": int(parts[2]),
                        "mem_total": int(parts[3]),
                        "gpu_bus_id": parts[4],
                        "processes": bus_id_to_pids.get(parts[4], []),
                    }
                    gpus.append(gpu_info)
                except (ValueError, IndexError) as e:
                    logger.error(f"GPU 数据解析失败: {e}, line: {line}")
                    continue
            return gpus

        except Exception as e:
            logger.error(f"获取 GPU 状态异常: {e}")
            return []

    @staticmethod
    def acquire_gpu_lock(gpu_indices):
        """原子性获取 GPU 锁"""
        acquired = []
        for gpu_idx in gpu_indices:
            lock_file = BASE_DIR / f"gpu_{gpu_idx}.lock"
            retries = 0

            while retries < MAX_RETRIES:
                try:
                    with open(lock_file, "w+") as f:
                        try:
                            fcntl.flock(f, fcntl.LOCK_EX | fcntl.LOCK_NB)
                            f.write(f"{os.getpid()} {time.time()}")
                            f.flush()
                            acquired.append(gpu_idx)
                            break
                        except BlockingIOError:
                            pass  # 锁被其他进程占用
                except OSError as e:
                    logger.warning(f"获取 GPU {gpu_idx} 锁失败: {e}")
                time.sleep(0.1)
                retries += 1

            if gpu_idx not in acquired:
                GPUManager.release_gpu_lock(acquired)
                return False
        return True

    @staticmethod
    def release_gpu_lock(gpu_indices):
        """安全释放 GPU 锁"""
        for gpu_idx in gpu_indices:
            lock_file = BASE_DIR / f"gpu_{gpu_idx}.lock"
            try:
                if lock_file.exists():
                    with open(lock_file, "r+") as f:
                        try:
                            fcntl.flock(f, fcntl.LOCK_EX)
                            content = f.read().strip()
                            if content.startswith(str(os.getpid())):
                                f.seek(0)
                                f.truncate()
                                fcntl.flock(f, fcntl.LOCK_UN)
                                os.remove(lock_file)
                            else:
                                logger.warning(
                                    f"进程 {os.getpid()} 尝试释放不属于自己的 GPU {gpu_idx} 锁"
                                )
                        except Exception as e:
                            logger.error(f"释放 GPU {gpu_idx} 锁时发生内部错误: {e}")
            except FileNotFoundError:
                logger.warning(f"GPU {gpu_idx} 的锁文件不存在，可能已被意外删除")
            except Exception as e:
                logger.warning(f"释放 GPU {gpu_idx} 锁失败: {e}")


class Task:
    def __init__(self, task_id, cmd, gpus, user):
        self.id = task_id
        self.cmd = cmd
        self.gpus = gpus
        self.user = user
        self.status = "QUEUED"
        self.pid = None
        self.exit_code = None
        self.proc = None
        self.start_time = None
        self.end_time = None

    def duration(self):
        """计算任务运行时间"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return None


class TaskScheduler:
    def __init__(self):
        self.task_queue = deque()
        self.running = {}
        self.lock = threading.Lock()
        self.task_id = 0
        self.activate = True
        self.condition = threading.Condition(self.lock)
        self.total_gpus = len(GPUManager.get_gpu_status())
        self.shutdown_requested = False
        self.thread = threading.Thread(
            target=self.schedule_loop, daemon=True
        )  # 设置为守护线程
        self.thread.start()

    def schedule_loop(self):
        """任务调度主循环"""
        while not self.shutdown_requested:
            with self.lock:
                self._process_completed_tasks()
                self._schedule_pending_tasks()
                sleep_time = 1 if self.task_queue else 5
                self.condition.wait(timeout=sleep_time)

    def _process_completed_tasks(self):
        """处理已完成任务"""
        completed_pids = []
        for pid, task in list(self.running.items()):
            if task.proc.poll() is not None:
                task.end_time = time.time()
                task.exit_code = task.proc.returncode
                task.status = "COMPLETED" if task.exit_code == 0 else "FAILED"
                GPUManager.release_gpu_lock(task.gpus)
                completed_pids.append(pid)
                logger.info(f"任务 {task.id} 完成，退出码: {task.exit_code}")

        for pid in completed_pids:
            del self.running[pid]

    def _schedule_pending_tasks(self):
        """调度等待中的任务"""
        available_gpus = self.get_available_gpus()
        while self.task_queue and len(available_gpus) >= self.task_queue[0].gpus:
            task = self.task_queue.popleft()
            required_gpus = task.gpus

            if required_gpus > self.total_gpus:
                logger.error(
                    f"任务 {task.id} 需要 {required_gpus} 个 GPU，超过系统总量"
                )
                task.status = "FAILED"
                continue

            allocated = available_gpus[:required_gpus]
            if GPUManager.acquire_gpu_lock(allocated):
                self.start_task(task, allocated)
                available_gpus = available_gpus[required_gpus:]
            else:
                logger.warning(f"任务 {task.id} 获取 GPU 锁失败，将重新放回队列")
                self.task_queue.appendleft(task)
                break

    def get_available_gpus(self):
        """获取可用 GPU 列表"""
        available = []
        for gpu in GPUManager.get_gpu_status():
            lock_file = BASE_DIR / f"gpu_{gpu['index']}.lock"
            if not lock_file.exists():
                available.append(gpu["index"])
        return sorted(available)

    def start_task(self, task, gpus):
        """启动任务并绑定 GPU"""
        try:
            env = os.environ.copy()
            env["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, gpus))
            task.start_time = time.time()
            task.status = "RUNNING"
            task.proc = subprocess.Popen(
                task.cmd,
                env=env,
                start_new_session=True,
            )
            task.pid = task.proc.pid
            task.gpus = gpus
            self.running[task.pid] = task
            logger.info(f"任务 {task.id} 启动，PID: {task.pid}，使用 GPU: {gpus}")
        except Exception as e:
            logger.error(f"任务 {task.id} 启动失败: {e}")
            task.status = "FAILED"
            GPUManager.release_gpu_lock(gpus)
            self.task_queue.appendleft(task)

    def list_queued_tasks(self):
        with self.lock:
            return list(self.task_queue)

    def delete_task(self, task_id):
        with self.lock:
            for i, task in enumerate(self.task_queue):
                if task.id == task_id:
                    del self.task_queue[i]
                    logger.info(f"任务 {task_id} 已删除")
                    return True
            return False

    def shutdown(self):
        """优雅终止所有任务"""
        self.shutdown_requested = True
        with self.lock:
            self.activate = False
            self.condition.notify_all()

        # 终止所有运行中的任务
        for task in list(self.running.values()):
            try:
                os.killpg(os.getpgid(task.pid), signal.SIGTERM)
                task.proc.wait(timeout=5)
            except subprocess.TimeoutExpired:
                os.killpg(os.getpgid(task.pid), signal.SIGKILL)
            except ProcessLookupError:
                pass
            finally:
                GPUManager.release_gpu_lock(task.gpus)
                if task.pid in self.running:
                    del self.running[task.pid]

        # 等待调度线程结束
        self.thread.join(timeout=10)


def run_client(args, scheduler):
    try:
        if args.version:
            print(f"Orun 版本: {VERSION}")
            return

        if args.status:
            gpus = GPUManager.get_gpu_status()
            running = len(scheduler.running)
            queued = len(scheduler.task_queue)
            print(f"总 GPU 数: {len(gpus)}")
            print(f"可用 GPU: {scheduler.get_available_gpus()}")
            print(f"运行中任务: {running}")
            print(f"排队任务: {queued}")
            return

        if args.list:
            tasks = scheduler.list_queued_tasks()
            if not tasks:
                print("队列中无任务")
                return
            print("{:<5} {:<10} {:<30} {:<10}".format("ID", "用户", "命令", "GPU需求"))
            print("-" * 60)
            for task in tasks:
                cmd = " ".join(task.cmd)[:30]
                print(
                    "{:<5} {:<10} {:<30} {:<10}".format(
                        task.id, task.user, cmd, task.gpus
                    )
                )
            return

        if args.delete is not None:
            if scheduler.delete_task(args.delete):
                print(f"任务 {args.delete} 已删除")
            else:
                print(f"任务 {args.delete} 不存在")
            return

        if args.command:
            with scheduler.lock:
                scheduler.task_id += 1
                task = Task(
                    task_id=scheduler.task_id,
                    cmd=args.command,
                    gpus=args.gpus,
                    user=os.getenv("USER", "unknown"),
                )
                if task.gpus > scheduler.total_gpus:
                    logger.error(
                        f"请求的 GPU 数 ({task.gpus}) 超过系统总量 ({scheduler.total_gpus})"
                    )
                    return
                scheduler.task_queue.append(task)
                scheduler.condition.notify()
                print(f"任务 {task.id} 已加入队列（需要 {task.gpus} 个 GPU）")

    except Exception as e:
        logger.error(f"客户端错误: {e}")
        scheduler.shutdown()


def main():
    parser = argparse.ArgumentParser(prog="orun", description="GPU 任务调度系统")
    parser.add_argument("command", nargs=argparse.REMAINDER, help="要执行的命令")
    parser.add_argument("--gpus", type=int, default=1, help="需要的 GPU 数量")
    parser.add_argument("--status", action="store_true", help="显示系统状态")
    parser.add_argument("--version", action="store_true", help="显示版本信息")
    parser.add_argument("--list", action="store_true", help="列出队列中的任务")
    parser.add_argument("--delete", type=int, help="删除指定 ID 的任务")
    args = parser.parse_args()

    if args.version:
        print(f"Orun 版本: {VERSION}")
        return

    if not args.command and not any([args.status, args.list, args.delete]):
        parser.print_help()
        return

    scheduler = TaskScheduler()

    # 注册信号处理
    def signal_handler(sig, frame):
        scheduler.shutdown()
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        run_client(args, scheduler)
        if args.command:
            # 保持主线程运行直到收到终止信号
            while not scheduler.shutdown_requested:
                time.sleep(1)
    except Exception as e:
        logger.error(f"主程序发生错误: {e}")
        scheduler.shutdown()
    finally:
        if (
            hasattr(scheduler, "shutdown_requested")
            and not scheduler.shutdown_requested
        ):
            scheduler.shutdown()


if __name__ == "__main__":
    main()
